We generate trajectory at the observation gap we have for our assimilation. We then generate observations from it and perform assimilnation.

* Methodology 
Goal: We want a good filter which does not fail for the setting below. 

Initial condition of the filter bias=6.0, covariance=4.0.

ob error cov=1.0 I
ob_gap=0.01, 0.05

Significance of 0.05 -> 
A high frequency limit. The lyapunov time scale is ~ 1.12( l_max= 0.9), which is 22 times the observation gap chosen. We chose to look at this values are experiments employ high-freqency measurement of available quanitity. Employing or measuring different variables are 
not possible, one some variables can be measured.

Note error covariance means error variance, as we have only y-coordinates being observed.
This is still very good.

Cases successful:
Single observation realizations

gap = 0.01, ob_cov=I,  -> N=40, alpha=1.1
gap = 0.05, ob_cov=I, 4I-> N=40, alpha=1.0

Observation covariance 4I should be more than sufficient error allowed for the system with just one scaler observation.

Excellent analysis, jacobian error controls the error in computing the clvs.

*L63
Generate a trajectory of physical time 500 with dt=0.01 and 0.05
Perform assimilation over full length. Note time when the error becomes bounded.

We use the part of trajectory after the error become bounded. Denote the time by t0. 
Use this part of trajectory as initial condition for the clv.

Notes: I chose y-coordinate for observation, as it is coupled to both x and z in the dynamics.
The frequency of observations is 0.01, which I think is high-frequency case, which is also
used in the paper. I tried different values of alpha, and arrived at the conclusion 
increasing the alpha was less useful than increasing the number of ensembles.

Next is to compute clvs along the analysis state. Same for the true state.

* Results
1. Understanding how to quantify the errors in clv is next important topic which we need to discuss.
Also, similar in the spirit of filter stability, we want to see how the clvs differ from one 
observation realization to another. This may generate statistics of the errors one may encounter.
But this being robust makes clvs of high practical utility. 

2. Another way is to look at the cosine of the angle between the precomputed clv and the clvs obtained
about the analysis state. We must also recover the exponents and other related properties.
The relative angle between the clvs is also important to understand.


* Possible future directions
Model errors in the parameters can also be addressed under the framework of filtering by including 
them in the state vector itself. A small experiment can be done. But this is the strength of the 
filtering process. Since jacobian may depend on parameters, it can use mean of the distribution of these parameters estimated over time.

Is there a possiblity of learning the operators for the clvs?
Knowing it over most of the attractor for one parameter, can one predict the change with
the change in parameters?